{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61688b3c-e40c-4200-9cb7-19da7868736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook varya is trying to export the median point (from the precipitation) from HMA - regions \n",
    "# to a dataframe and then later csv file to use as a forsing for testrun of sedcas model \n",
    "\n",
    "# take the locations determined in a different notebook (also stored in some csv file already)\n",
    "# loop through every file that is in the folder on the server:\n",
    "#    find location. select data \n",
    "#    \n",
    "\n",
    "# find that location in the HMA climate from the server \n",
    "# convert to csv \n",
    "# save \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e012c353-5572-49b5-8259-f5f4fd3c7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde9f3ac-c387-457f-9e2a-ec1248bb02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a090d2-dfe0-4526-b919-633c1c7d1c6a",
   "metadata": {},
   "source": [
    "## workapathround (make netcdf conversion to df faster): \n",
    "\n",
    "### (this is an important part) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b2b2d-ebad-4ce4-a590-04e2a45bb8ad",
   "metadata": {},
   "source": [
    "### locations from csv and .shp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df80741f-9a03-4f1d-8aea-95527e5d147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read locations file - for annual sum precipitations: \n",
    "locations_pr = pd.read_table('out/median_annual_precip_regions.csv', sep = ',',index_col =0)\n",
    "\n",
    "# read locations file - for annial median temp:\n",
    "locations_t2m = pd.read_table('out/annual_median_t2m_regions.csv', sep = ',', index_col = 0)\n",
    "\n",
    "# read hma polygons file to a df:\n",
    "hma = gpd.read_file('HMA_regions/HMA_regions.shp')\n",
    "hma_df = pd.DataFrame(hma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6817ec-e659-44d0-99e1-b0cea8fde8e3",
   "metadata": {},
   "source": [
    "### path to folders and output to where save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16e95e5-7cee-4d32-8635-c9771d8c544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_temp = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/2m-temperature/'\n",
    "\n",
    "# precipitations\n",
    "path_precip = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/'\n",
    "\n",
    "# total cloud cover \n",
    "path_clouds = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-cloud-cover/'\n",
    "\n",
    "# incoming radiation \n",
    "path_radiation = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/surface-solar-radiation-downwards/'\n",
    "\n",
    "\n",
    "# path for output timeseries\n",
    "path_output = \"/Users/varyabazilova/Desktop/hma_regions/out/timeseries\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad97a3-ac02-49e2-83f8-d18e22d9ef9c",
   "metadata": {},
   "source": [
    "### what mountain range are we looking at? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6220775-9960-44c8-b213-54b285507b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of mnt range: Eastern Tien Shan\n"
     ]
    }
   ],
   "source": [
    "# take the n index of the \"locations precipitation\" table \n",
    "\n",
    "n = 19\n",
    "print('name of mnt range:', hma_df.Name[n])\n",
    "\n",
    "point_lat = locations_pr.latitude[n]\n",
    "point_lon = locations_pr.longitude[n]\n",
    "\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3682942-ccc0-4978-aa9d-22f0d2f59dcc",
   "metadata": {},
   "source": [
    "### loop: get the csv for selected point (one mnt range) for each parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df821d1-1e60-40ea-966a-7f119fc4522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 2022-06-21 16:52:59.862544\n",
      "ready to turn into df at: 2022-06-21 16:53:00.022725\n",
      "year - done: 1979\n",
      "done at: 2022-06-21 16:53:36.610193\n",
      "ready to turn into df at: 2022-06-21 16:53:36.648649\n",
      "year - done: 1980\n",
      "done at: 2022-06-21 16:53:50.095908\n",
      "ready to turn into df at: 2022-06-21 16:53:50.127693\n",
      "year - done: 1981\n",
      "done at: 2022-06-21 16:54:04.522725\n",
      "ready to turn into df at: 2022-06-21 16:54:04.588292\n",
      "year - done: 1982\n",
      "done at: 2022-06-21 16:54:20.696882\n",
      "ready to turn into df at: 2022-06-21 16:54:20.724529\n",
      "year - done: 1983\n",
      "done at: 2022-06-21 16:54:54.084248\n",
      "ready to turn into df at: 2022-06-21 16:54:54.131078\n",
      "year - done: 1984\n",
      "done at: 2022-06-21 16:55:23.519231\n",
      "ready to turn into df at: 2022-06-21 16:55:23.554896\n",
      "year - done: 1985\n",
      "done at: 2022-06-21 16:55:53.480138\n",
      "ready to turn into df at: 2022-06-21 16:55:53.539037\n",
      "year - done: 1986\n",
      "done at: 2022-06-21 16:56:12.039652\n",
      "ready to turn into df at: 2022-06-21 16:56:12.075404\n",
      "year - done: 1987\n",
      "done at: 2022-06-21 16:56:37.223991\n",
      "ready to turn into df at: 2022-06-21 16:56:37.255911\n",
      "year - done: 1988\n",
      "done at: 2022-06-21 16:57:11.487191\n",
      "ready to turn into df at: 2022-06-21 16:57:11.525916\n",
      "year - done: 1989\n",
      "done at: 2022-06-21 16:57:27.685648\n",
      "ready to turn into df at: 2022-06-21 16:57:27.707439\n",
      "year - done: 1990\n",
      "done at: 2022-06-21 16:57:43.018602\n",
      "ready to turn into df at: 2022-06-21 16:57:43.055028\n",
      "year - done: 1991\n",
      "done at: 2022-06-21 16:58:14.243851\n",
      "ready to turn into df at: 2022-06-21 16:58:14.293048\n",
      "year - done: 1992\n",
      "done at: 2022-06-21 16:58:47.840046\n",
      "ready to turn into df at: 2022-06-21 16:58:47.868318\n",
      "year - done: 1993\n",
      "done at: 2022-06-21 16:59:24.764516\n",
      "ready to turn into df at: 2022-06-21 16:59:24.817382\n",
      "year - done: 1994\n",
      "done at: 2022-06-21 16:59:40.524183\n",
      "ready to turn into df at: 2022-06-21 16:59:40.560349\n",
      "year - done: 1995\n",
      "done at: 2022-06-21 17:00:02.405768\n",
      "ready to turn into df at: 2022-06-21 17:00:02.454659\n",
      "year - done: 1996\n",
      "done at: 2022-06-21 17:00:35.753616\n",
      "ready to turn into df at: 2022-06-21 17:00:35.795054\n",
      "year - done: 1997\n",
      "done at: 2022-06-21 17:00:51.984366\n",
      "ready to turn into df at: 2022-06-21 17:00:52.012571\n",
      "year - done: 1998\n",
      "done at: 2022-06-21 17:01:24.467014\n",
      "ready to turn into df at: 2022-06-21 17:01:24.500565\n",
      "year - done: 1999\n",
      "done at: 2022-06-21 17:01:58.500349\n",
      "ready to turn into df at: 2022-06-21 17:01:58.547991\n",
      "year - done: 2000\n",
      "done at: 2022-06-21 17:02:34.904633\n",
      "ready to turn into df at: 2022-06-21 17:02:34.922701\n",
      "year - done: 2001\n",
      "done at: 2022-06-21 17:03:07.688057\n",
      "ready to turn into df at: 2022-06-21 17:03:07.726771\n",
      "year - done: 2002\n",
      "done at: 2022-06-21 17:03:23.313620\n",
      "ready to turn into df at: 2022-06-21 17:03:23.329777\n",
      "year - done: 2003\n",
      "done at: 2022-06-21 17:03:56.405758\n",
      "ready to turn into df at: 2022-06-21 17:03:56.423808\n",
      "year - done: 2004\n",
      "done at: 2022-06-21 17:04:13.119197\n",
      "ready to turn into df at: 2022-06-21 17:04:13.135073\n",
      "year - done: 2005\n",
      "done at: 2022-06-21 17:04:47.029928\n",
      "ready to turn into df at: 2022-06-21 17:04:47.046729\n",
      "year - done: 2006\n",
      "done at: 2022-06-21 17:05:15.880663\n",
      "ready to turn into df at: 2022-06-21 17:05:15.926528\n",
      "year - done: 2007\n",
      "done at: 2022-06-21 17:05:46.886073\n",
      "ready to turn into df at: 2022-06-21 17:05:46.908645\n",
      "year - done: 2008\n",
      "done at: 2022-06-21 17:06:15.387148\n",
      "ready to turn into df at: 2022-06-21 17:06:15.417939\n",
      "year - done: 2009\n",
      "done at: 2022-06-21 17:06:52.347435\n",
      "ready to turn into df at: 2022-06-21 17:06:52.379563\n",
      "year - done: 2010\n",
      "done at: 2022-06-21 17:07:06.870689\n",
      "ready to turn into df at: 2022-06-21 17:07:06.915895\n",
      "year - done: 2011\n",
      "done at: 2022-06-21 17:07:22.915256\n",
      "ready to turn into df at: 2022-06-21 17:07:22.932134\n",
      "year - done: 2012\n",
      "done at: 2022-06-21 17:07:56.743428\n",
      "ready to turn into df at: 2022-06-21 17:07:56.778633\n",
      "year - done: 2013\n",
      "done at: 2022-06-21 17:08:18.600014\n",
      "ready to turn into df at: 2022-06-21 17:08:18.617305\n",
      "year - done: 2014\n",
      "done at: 2022-06-21 17:08:32.396269\n",
      "ready to turn into df at: 2022-06-21 17:08:32.414204\n",
      "year - done: 2015\n",
      "done at: 2022-06-21 17:08:53.487838\n",
      "ready to turn into df at: 2022-06-21 17:08:53.505774\n",
      "year - done: 2016\n",
      "done at: 2022-06-21 17:09:28.691243\n",
      "ready to turn into df at: 2022-06-21 17:09:28.729423\n",
      "year - done: 2017\n",
      "done at: 2022-06-21 17:09:43.412285\n",
      "ready to turn into df at: 2022-06-21 17:09:43.447418\n",
      "year - done: 2018\n",
      "done at: 2022-06-21 17:09:59.218736\n",
      "ready to turn into df at: 2022-06-21 17:09:59.249978\n",
      "year - done: 2019\n",
      "done at: 2022-06-21 17:10:15.039144\n",
      "ready to turn into df at: 2022-06-21 17:10:15.077204\n",
      "year - done: 2020\n",
      "done at: 2022-06-21 17:11:22.054552\n",
      "CPU times: user 2.84 s, sys: 45.2 s, total: 48.1 s\n",
      "Wall time: 18min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# temperatures\n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_temp)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    # ds_df.to_csv(path_output + '_temp_{y}.csv'.format(y=year))\n",
    "    ds_df.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/' + 'timeseries_temp_{y}.csv'.format(y=year))\n",
    "\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3493647-2c82-44f6-b13c-bf8cc8c07357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test cell to see what am i doing wrong :/ \n",
    "'''\n",
    "%%time \n",
    "preciptest = xr.open_dataset('/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/era5_total-precipitation_hourly_1979.nc')\n",
    "# preciptest2 = xr.open_dataset('/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/era5_total-precipitation_hourly_1980.nc')\n",
    "\n",
    "\n",
    "# ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "ds = preciptest.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1515c-7d7b-4659-b4c1-ef250c33e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_df = ds_df.reset_index()#.time.values\n",
    "\n",
    "# for x in preciptest.time.values:\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1d3c0-59b4-4124-97f1-b98b2bf39570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_df = ds_df.sort_values('time') \n",
    "# for x in ds_df.time.values:\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733ea5d-f495-46ef-a5e6-b31e8a746e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f132422-fcff-447b-b082-d6d529fe6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 2022-06-21 17:11:22.060121\n",
      "ready to turn into df at: 2022-06-21 17:11:22.102816\n",
      "year - done: 1979\n",
      "done at: 2022-06-21 17:11:38.013408\n",
      "ready to turn into df at: 2022-06-21 17:11:38.028999\n",
      "year - done: 1980\n",
      "done at: 2022-06-21 17:12:13.880915\n",
      "ready to turn into df at: 2022-06-21 17:12:13.922335\n",
      "year - done: 1981\n",
      "done at: 2022-06-21 17:12:26.757820\n",
      "ready to turn into df at: 2022-06-21 17:12:26.787670\n",
      "year - done: 1982\n",
      "done at: 2022-06-21 17:12:41.137756\n",
      "ready to turn into df at: 2022-06-21 17:12:41.154195\n",
      "year - done: 1983\n",
      "done at: 2022-06-21 17:12:55.355254\n",
      "ready to turn into df at: 2022-06-21 17:12:55.378842\n",
      "year - done: 1984\n",
      "done at: 2022-06-21 17:13:12.240162\n",
      "ready to turn into df at: 2022-06-21 17:13:12.262427\n",
      "year - done: 1985\n",
      "done at: 2022-06-21 17:13:46.807719\n",
      "ready to turn into df at: 2022-06-21 17:13:46.843806\n",
      "year - done: 1986\n",
      "done at: 2022-06-21 17:14:03.101076\n",
      "ready to turn into df at: 2022-06-21 17:14:03.116616\n",
      "year - done: 1987\n",
      "done at: 2022-06-21 17:14:18.649789\n",
      "ready to turn into df at: 2022-06-21 17:14:18.671544\n",
      "year - done: 1988\n",
      "done at: 2022-06-21 17:14:33.559080\n",
      "ready to turn into df at: 2022-06-21 17:14:33.575878\n",
      "year - done: 1989\n",
      "done at: 2022-06-21 17:14:48.031629\n",
      "ready to turn into df at: 2022-06-21 17:14:48.044712\n",
      "year - done: 1990\n",
      "done at: 2022-06-21 17:15:03.576772\n",
      "ready to turn into df at: 2022-06-21 17:15:03.604139\n",
      "year - done: 1991\n",
      "done at: 2022-06-21 17:15:18.908868\n",
      "ready to turn into df at: 2022-06-21 17:15:18.937044\n",
      "year - done: 1992\n",
      "done at: 2022-06-21 17:15:53.592740\n",
      "ready to turn into df at: 2022-06-21 17:15:53.615376\n",
      "year - done: 1993\n",
      "done at: 2022-06-21 17:16:29.012360\n",
      "ready to turn into df at: 2022-06-21 17:16:29.043728\n",
      "year - done: 1994\n",
      "done at: 2022-06-21 17:17:05.233034\n",
      "ready to turn into df at: 2022-06-21 17:17:05.250832\n",
      "year - done: 1995\n",
      "done at: 2022-06-21 17:17:38.593566\n",
      "ready to turn into df at: 2022-06-21 17:17:38.610562\n",
      "year - done: 1996\n",
      "done at: 2022-06-21 17:18:12.404410\n",
      "ready to turn into df at: 2022-06-21 17:18:12.424810\n",
      "year - done: 1997\n",
      "done at: 2022-06-21 17:18:28.158754\n",
      "ready to turn into df at: 2022-06-21 17:18:28.183404\n",
      "year - done: 1998\n",
      "done at: 2022-06-21 17:19:03.483939\n",
      "ready to turn into df at: 2022-06-21 17:19:03.526297\n",
      "year - done: 1999\n",
      "done at: 2022-06-21 17:19:18.977833\n",
      "ready to turn into df at: 2022-06-21 17:19:19.024855\n",
      "year - done: 2000\n",
      "done at: 2022-06-21 17:19:36.020504\n",
      "ready to turn into df at: 2022-06-21 17:19:36.044439\n",
      "year - done: 2001\n",
      "done at: 2022-06-21 17:19:52.662082\n",
      "ready to turn into df at: 2022-06-21 17:19:52.677723\n",
      "year - done: 2002\n",
      "done at: 2022-06-21 17:20:26.967597\n",
      "ready to turn into df at: 2022-06-21 17:20:27.003015\n",
      "year - done: 2003\n",
      "done at: 2022-06-21 17:20:41.679328\n",
      "ready to turn into df at: 2022-06-21 17:20:41.695371\n",
      "year - done: 2004\n",
      "done at: 2022-06-21 17:21:13.817689\n",
      "ready to turn into df at: 2022-06-21 17:21:13.851016\n",
      "year - done: 2005\n",
      "done at: 2022-06-21 17:21:29.088667\n",
      "ready to turn into df at: 2022-06-21 17:21:29.121856\n",
      "year - done: 2006\n",
      "done at: 2022-06-21 17:22:01.761166\n",
      "ready to turn into df at: 2022-06-21 17:22:01.795882\n",
      "year - done: 2007\n",
      "done at: 2022-06-21 17:22:17.659517\n",
      "ready to turn into df at: 2022-06-21 17:22:17.687817\n",
      "year - done: 2008\n",
      "done at: 2022-06-21 17:22:43.654082\n",
      "ready to turn into df at: 2022-06-21 17:22:43.702717\n",
      "year - done: 2009\n",
      "done at: 2022-06-21 17:22:59.756295\n",
      "ready to turn into df at: 2022-06-21 17:22:59.772486\n",
      "year - done: 2010\n",
      "done at: 2022-06-21 17:23:37.455641\n",
      "ready to turn into df at: 2022-06-21 17:23:37.473125\n",
      "year - done: 2011\n",
      "done at: 2022-06-21 17:23:52.788347\n",
      "ready to turn into df at: 2022-06-21 17:23:52.805630\n",
      "year - done: 2012\n",
      "done at: 2022-06-21 17:24:25.797153\n",
      "ready to turn into df at: 2022-06-21 17:24:25.828134\n",
      "year - done: 2013\n",
      "done at: 2022-06-21 17:24:43.338451\n",
      "ready to turn into df at: 2022-06-21 17:24:43.354447\n",
      "year - done: 2014\n",
      "done at: 2022-06-21 17:25:20.888302\n",
      "ready to turn into df at: 2022-06-21 17:25:20.922717\n",
      "year - done: 2015\n",
      "done at: 2022-06-21 17:26:00.239581\n",
      "ready to turn into df at: 2022-06-21 17:26:00.263947\n",
      "year - done: 2016\n",
      "done at: 2022-06-21 17:26:39.658901\n",
      "ready to turn into df at: 2022-06-21 17:26:39.687468\n",
      "year - done: 2017\n",
      "done at: 2022-06-21 17:27:18.388250\n",
      "ready to turn into df at: 2022-06-21 17:27:18.405602\n",
      "year - done: 2018\n",
      "done at: 2022-06-21 17:27:53.662862\n",
      "ready to turn into df at: 2022-06-21 17:27:53.686451\n",
      "year - done: 2019\n",
      "done at: 2022-06-21 17:28:08.410730\n",
      "ready to turn into df at: 2022-06-21 17:28:08.444728\n",
      "year - done: 2020\n",
      "done at: 2022-06-21 17:28:41.523790\n",
      "CPU times: user 2.7 s, sys: 42.9 s, total: 45.6 s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# precipitation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_precip)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude'])#.squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    # ds_df.to_csv(path_output + '_precip_{y}.csv'.format(y=year))\n",
    "    ds_df.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/' + 'timeseries_precip_{y}.csv'.format(y=year))\n",
    "\n",
    "\n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3ffdde-f91e-4aae-a34c-c1fcf93dad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 2022-06-21 17:28:41.533251\n",
      "ready to turn into df at: 2022-06-21 17:28:41.553096\n",
      "year - done: 1979\n",
      "done at: 2022-06-21 17:29:05.881517\n",
      "ready to turn into df at: 2022-06-21 17:29:05.903719\n",
      "year - done: 1980\n",
      "done at: 2022-06-21 17:29:40.788166\n",
      "ready to turn into df at: 2022-06-21 17:29:40.810922\n",
      "year - done: 1981\n",
      "done at: 2022-06-21 17:30:09.265628\n",
      "ready to turn into df at: 2022-06-21 17:30:09.285756\n",
      "year - done: 1982\n",
      "done at: 2022-06-21 17:30:54.141939\n",
      "ready to turn into df at: 2022-06-21 17:30:54.163159\n",
      "year - done: 1983\n",
      "done at: 2022-06-21 17:31:18.307749\n",
      "ready to turn into df at: 2022-06-21 17:31:18.328430\n",
      "year - done: 1984\n",
      "done at: 2022-06-21 17:31:34.176919\n",
      "ready to turn into df at: 2022-06-21 17:31:34.192777\n",
      "year - done: 1985\n",
      "done at: 2022-06-21 17:32:13.621575\n",
      "ready to turn into df at: 2022-06-21 17:32:13.638189\n",
      "year - done: 1986\n",
      "done at: 2022-06-21 17:32:30.531513\n",
      "ready to turn into df at: 2022-06-21 17:32:30.548455\n",
      "year - done: 1987\n",
      "done at: 2022-06-21 17:32:45.528254\n",
      "ready to turn into df at: 2022-06-21 17:32:45.546453\n",
      "year - done: 1988\n",
      "done at: 2022-06-21 17:33:03.888025\n",
      "ready to turn into df at: 2022-06-21 17:33:03.903159\n",
      "year - done: 1989\n",
      "done at: 2022-06-21 17:33:20.881389\n",
      "ready to turn into df at: 2022-06-21 17:33:20.898172\n",
      "year - done: 1990\n",
      "done at: 2022-06-21 17:33:36.179147\n",
      "ready to turn into df at: 2022-06-21 17:33:36.195225\n",
      "year - done: 1991\n",
      "done at: 2022-06-21 17:33:52.099697\n",
      "ready to turn into df at: 2022-06-21 17:33:52.115126\n",
      "year - done: 1992\n",
      "done at: 2022-06-21 17:34:07.286896\n",
      "ready to turn into df at: 2022-06-21 17:34:07.305387\n",
      "year - done: 1993\n",
      "done at: 2022-06-21 17:34:25.111524\n",
      "ready to turn into df at: 2022-06-21 17:34:25.127381\n",
      "year - done: 1994\n",
      "done at: 2022-06-21 17:35:07.174224\n",
      "ready to turn into df at: 2022-06-21 17:35:07.191687\n",
      "year - done: 1995\n",
      "done at: 2022-06-21 17:35:42.883974\n",
      "ready to turn into df at: 2022-06-21 17:35:42.901100\n",
      "year - done: 1996\n",
      "done at: 2022-06-21 17:36:00.127613\n",
      "ready to turn into df at: 2022-06-21 17:36:00.143127\n",
      "year - done: 1997\n",
      "done at: 2022-06-21 17:36:30.500336\n",
      "ready to turn into df at: 2022-06-21 17:36:30.518039\n",
      "year - done: 1998\n",
      "done at: 2022-06-21 17:36:45.840569\n",
      "ready to turn into df at: 2022-06-21 17:36:45.861320\n",
      "year - done: 1999\n",
      "done at: 2022-06-21 17:37:01.207239\n",
      "ready to turn into df at: 2022-06-21 17:37:01.222673\n",
      "year - done: 2000\n",
      "done at: 2022-06-21 17:37:34.819152\n",
      "ready to turn into df at: 2022-06-21 17:37:34.841492\n",
      "year - done: 2001\n",
      "done at: 2022-06-21 17:37:48.861729\n",
      "ready to turn into df at: 2022-06-21 17:37:48.879184\n",
      "year - done: 2002\n",
      "done at: 2022-06-21 17:38:04.418368\n",
      "ready to turn into df at: 2022-06-21 17:38:04.434622\n",
      "year - done: 2003\n",
      "done at: 2022-06-21 17:38:17.790877\n",
      "ready to turn into df at: 2022-06-21 17:38:17.807895\n",
      "year - done: 2004\n",
      "done at: 2022-06-21 17:38:33.106848\n",
      "ready to turn into df at: 2022-06-21 17:38:33.125833\n",
      "year - done: 2005\n",
      "done at: 2022-06-21 17:38:47.576772\n",
      "ready to turn into df at: 2022-06-21 17:38:47.594879\n",
      "year - done: 2006\n",
      "done at: 2022-06-21 17:39:02.517386\n",
      "ready to turn into df at: 2022-06-21 17:39:02.534048\n",
      "year - done: 2007\n",
      "done at: 2022-06-21 17:39:19.080793\n",
      "ready to turn into df at: 2022-06-21 17:39:19.098361\n",
      "year - done: 2008\n",
      "done at: 2022-06-21 17:39:34.935065\n",
      "ready to turn into df at: 2022-06-21 17:39:34.954605\n",
      "year - done: 2009\n",
      "done at: 2022-06-21 17:39:57.745155\n",
      "ready to turn into df at: 2022-06-21 17:39:57.759298\n",
      "year - done: 2010\n",
      "done at: 2022-06-21 17:40:11.688954\n",
      "ready to turn into df at: 2022-06-21 17:40:11.705124\n",
      "year - done: 2011\n",
      "done at: 2022-06-21 17:40:26.747437\n",
      "ready to turn into df at: 2022-06-21 17:40:26.762705\n",
      "year - done: 2012\n",
      "done at: 2022-06-21 17:40:41.619132\n",
      "ready to turn into df at: 2022-06-21 17:40:41.635194\n",
      "year - done: 2013\n",
      "done at: 2022-06-21 17:40:56.569263\n",
      "ready to turn into df at: 2022-06-21 17:40:56.586518\n",
      "year - done: 2014\n",
      "done at: 2022-06-21 17:41:13.265498\n",
      "ready to turn into df at: 2022-06-21 17:41:13.283174\n",
      "year - done: 2015\n",
      "done at: 2022-06-21 17:41:27.360342\n",
      "ready to turn into df at: 2022-06-21 17:41:27.379982\n",
      "year - done: 2016\n",
      "done at: 2022-06-21 17:41:41.343887\n",
      "ready to turn into df at: 2022-06-21 17:41:41.359696\n",
      "year - done: 2017\n",
      "done at: 2022-06-21 17:41:55.374157\n",
      "ready to turn into df at: 2022-06-21 17:41:55.391018\n",
      "year - done: 2018\n",
      "done at: 2022-06-21 17:42:26.437004\n",
      "ready to turn into df at: 2022-06-21 17:42:26.453607\n",
      "year - done: 2019\n",
      "done at: 2022-06-21 17:42:50.546032\n",
      "ready to turn into df at: 2022-06-21 17:42:50.565073\n",
      "year - done: 2020\n",
      "done at: 2022-06-21 17:43:20.163576\n",
      "CPU times: user 2.5 s, sys: 43.5 s, total: 46 s\n",
      "Wall time: 14min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# clouds \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_clouds)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    # ds_df.to_csv(path_output + '_cloud_cover_{y}.csv'.format(y=year))\n",
    "    ds_df.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/' + 'timeseries_clouds_{y}.csv'.format(y=year))\n",
    "\n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d80e5d3-3802-4f0f-83a4-09445b503d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 2022-06-21 17:43:20.172774\n",
      "ready to turn into df at: 2022-06-21 17:43:20.221953\n",
      "year - done: 1979\n",
      "done at: 2022-06-21 17:43:36.675476\n",
      "ready to turn into df at: 2022-06-21 17:43:36.692035\n",
      "year - done: 1980\n",
      "done at: 2022-06-21 17:44:07.130942\n",
      "ready to turn into df at: 2022-06-21 17:44:07.148224\n",
      "year - done: 1981\n",
      "done at: 2022-06-21 17:44:45.815515\n",
      "ready to turn into df at: 2022-06-21 17:44:45.833107\n",
      "year - done: 1982\n",
      "done at: 2022-06-21 17:45:18.657007\n",
      "ready to turn into df at: 2022-06-21 17:45:18.674098\n",
      "year - done: 1983\n",
      "done at: 2022-06-21 17:45:51.359792\n",
      "ready to turn into df at: 2022-06-21 17:45:51.378224\n",
      "year - done: 1984\n",
      "done at: 2022-06-21 17:46:24.553061\n",
      "ready to turn into df at: 2022-06-21 17:46:24.570789\n",
      "year - done: 1985\n",
      "done at: 2022-06-21 17:46:57.434622\n",
      "ready to turn into df at: 2022-06-21 17:46:57.454674\n",
      "year - done: 1986\n",
      "done at: 2022-06-21 17:47:13.496709\n",
      "ready to turn into df at: 2022-06-21 17:47:13.513729\n",
      "year - done: 1987\n",
      "done at: 2022-06-21 17:47:48.685681\n",
      "ready to turn into df at: 2022-06-21 17:47:48.705711\n",
      "year - done: 1988\n",
      "done at: 2022-06-21 17:48:04.395063\n",
      "ready to turn into df at: 2022-06-21 17:48:04.411774\n",
      "year - done: 1989\n",
      "done at: 2022-06-21 17:48:36.030298\n",
      "ready to turn into df at: 2022-06-21 17:48:36.048147\n",
      "year - done: 1990\n",
      "done at: 2022-06-21 17:49:12.991762\n",
      "ready to turn into df at: 2022-06-21 17:49:13.008709\n",
      "year - done: 1991\n",
      "done at: 2022-06-21 17:49:28.069283\n",
      "ready to turn into df at: 2022-06-21 17:49:28.089197\n",
      "year - done: 1992\n",
      "done at: 2022-06-21 17:50:00.931718\n",
      "ready to turn into df at: 2022-06-21 17:50:00.949549\n",
      "year - done: 1993\n",
      "done at: 2022-06-21 17:50:37.743212\n",
      "ready to turn into df at: 2022-06-21 17:50:37.763191\n",
      "year - done: 1994\n",
      "done at: 2022-06-21 17:51:10.964770\n",
      "ready to turn into df at: 2022-06-21 17:51:10.986771\n",
      "year - done: 1995\n",
      "done at: 2022-06-21 17:51:44.931518\n",
      "ready to turn into df at: 2022-06-21 17:51:44.953858\n",
      "year - done: 1996\n",
      "done at: 2022-06-21 17:52:16.501544\n",
      "ready to turn into df at: 2022-06-21 17:52:16.520191\n",
      "year - done: 1997\n",
      "done at: 2022-06-21 17:52:33.041662\n",
      "ready to turn into df at: 2022-06-21 17:52:33.056312\n",
      "year - done: 1998\n",
      "done at: 2022-06-21 17:53:03.318679\n",
      "ready to turn into df at: 2022-06-21 17:53:03.336100\n",
      "year - done: 1999\n",
      "done at: 2022-06-21 17:53:40.653770\n",
      "ready to turn into df at: 2022-06-21 17:53:40.671123\n",
      "year - done: 2000\n",
      "done at: 2022-06-21 17:54:11.101402\n",
      "ready to turn into df at: 2022-06-21 17:54:11.121237\n",
      "year - done: 2001\n",
      "done at: 2022-06-21 17:54:25.209553\n",
      "ready to turn into df at: 2022-06-21 17:54:25.225197\n",
      "year - done: 2002\n",
      "done at: 2022-06-21 17:54:39.589770\n",
      "ready to turn into df at: 2022-06-21 17:54:39.609902\n",
      "year - done: 2003\n",
      "done at: 2022-06-21 17:54:53.116682\n",
      "ready to turn into df at: 2022-06-21 17:54:53.131385\n",
      "year - done: 2004\n",
      "done at: 2022-06-21 17:55:07.235500\n",
      "ready to turn into df at: 2022-06-21 17:55:07.250620\n",
      "year - done: 2005\n",
      "done at: 2022-06-21 17:55:21.443293\n",
      "ready to turn into df at: 2022-06-21 17:55:21.461219\n",
      "year - done: 2006\n",
      "done at: 2022-06-21 17:55:36.473008\n",
      "ready to turn into df at: 2022-06-21 17:55:36.490571\n",
      "year - done: 2007\n",
      "done at: 2022-06-21 17:55:49.333772\n",
      "ready to turn into df at: 2022-06-21 17:55:49.350340\n",
      "year - done: 2008\n",
      "done at: 2022-06-21 17:56:04.279549\n",
      "ready to turn into df at: 2022-06-21 17:56:04.296625\n",
      "year - done: 2009\n",
      "done at: 2022-06-21 17:56:23.375939\n",
      "ready to turn into df at: 2022-06-21 17:56:23.396989\n",
      "year - done: 2010\n",
      "done at: 2022-06-21 17:56:59.526862\n",
      "ready to turn into df at: 2022-06-21 17:56:59.545751\n",
      "year - done: 2011\n",
      "done at: 2022-06-21 17:57:34.767242\n",
      "ready to turn into df at: 2022-06-21 17:57:34.783044\n",
      "year - done: 2012\n",
      "done at: 2022-06-21 17:58:03.926820\n",
      "ready to turn into df at: 2022-06-21 17:58:03.946201\n",
      "year - done: 2013\n",
      "done at: 2022-06-21 17:58:19.906986\n",
      "ready to turn into df at: 2022-06-21 17:58:19.925438\n",
      "year - done: 2014\n",
      "done at: 2022-06-21 17:58:36.363443\n",
      "ready to turn into df at: 2022-06-21 17:58:36.381605\n",
      "year - done: 2015\n",
      "done at: 2022-06-21 17:58:51.355509\n",
      "ready to turn into df at: 2022-06-21 17:58:51.373042\n",
      "year - done: 2016\n",
      "done at: 2022-06-21 17:59:06.307760\n",
      "ready to turn into df at: 2022-06-21 17:59:06.322839\n",
      "year - done: 2017\n",
      "done at: 2022-06-21 17:59:21.742747\n",
      "ready to turn into df at: 2022-06-21 17:59:21.757897\n",
      "year - done: 2018\n",
      "done at: 2022-06-21 17:59:36.656466\n",
      "ready to turn into df at: 2022-06-21 17:59:36.672376\n",
      "year - done: 2019\n",
      "done at: 2022-06-21 17:59:52.914565\n",
      "ready to turn into df at: 2022-06-21 17:59:52.941461\n",
      "year - done: 2020\n",
      "done at: 2022-06-21 18:00:23.178071\n",
      "CPU times: user 2.59 s, sys: 42.9 s, total: 45.5 s\n",
      "Wall time: 17min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# radiation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_radiation)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    # ds_df.to_csv(path_output + '_SWradiation_{y}.csv'.format(y=year))\n",
    "    ds_df.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/' + 'timeseries_radiation_{y}.csv'.format(y=year))\n",
    "\n",
    "\n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44af4f5-a22e-4a8e-9a31-f304055b9566",
   "metadata": {
    "tags": []
   },
   "source": [
    "### read created .csv-s and attach them together (separately for each variable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57cbf68b-9bb1-4f77-a8a2-8646d700e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 temperatures \n",
    "\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/temp/*.csv')\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/13_Tibetan_Interior_Mountains/temp/*.csv')\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/temp/*.csv')\n",
    "\n",
    "\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "temps_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# temps_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/temps_1979_2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ef846e-fc95-410f-a440-a68b84d5be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 precipitation \n",
    "\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/precip/*.csv')\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/13_Tibetan_Interior_Mountains/precip/*.csv')\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/precip/*.csv')\n",
    "\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "precip_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# precip_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/precip_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b86c605-2c56-482b-8293-4b75ea4b0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 clouds \n",
    "\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/clouds/*.csv')\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/13_Tibetan_Interior_Mountains/clouds/*.csv')\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/clouds/*.csv')\n",
    "\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "clouds_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# clouds_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/clouds_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc6271c-ec5f-4f57-946e-eb28b9ee0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 radiation \n",
    "\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/radiation/*.csv')\n",
    "# files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/13_Tibetan_Interior_Mountains/radiation/*.csv')\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/radiation/*.csv')\n",
    "\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "swradiation_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# swradiation_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/SWradiation_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389646e-1430-4f29-bdee-14a8acc664ff",
   "metadata": {},
   "source": [
    "### merge all variables together using 'time' as key \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16e307b8-4307-4884-9b0d-62cbca9e305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample climate data - how it all is supposed to look like:\n",
    "\n",
    "sampleclimate = pd.read_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/sample_climate.met', sep = '\\t')\n",
    "# sampleclimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c50d26d-7a4a-43cc-8470-5f8f42e5a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all \n",
    "path = '/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan'\n",
    "\n",
    "temps = pd.read_csv(path + '/temps_1979_2020.csv', index_col = 0)\n",
    "precip = pd.read_csv(path + '/precip_1979_2020.csv', index_col = 0)\n",
    "clouds = pd.read_csv(path + '/clouds_1979_2020.csv', index_col = 0)\n",
    "radiation = pd.read_csv(path + '/SWradiation_1979_2020.csv', index_col = 0)\n",
    "\n",
    "# slice data so it starts and ends at the same time\n",
    "start = '1980-01-01 00:00:00' \n",
    "end = '2019-01-01 00:00:00'\n",
    "\n",
    "temps = temps.loc[start:end,:]\n",
    "precip = precip.loc[start:end,:]\n",
    "clouds = clouds.loc[start:end,:]\n",
    "radiation = radiation.loc[start:end,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bc5c536-b5d0-4f84-8084-fc2d19a0e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps length: 341881\n",
      "precip length: 341881\n",
      "clouds length: 341881\n",
      "radiation length: 341881\n"
     ]
    }
   ],
   "source": [
    "print('temps length:', len(temps)) \n",
    "print('precip length:', len(precip)) \n",
    "print('clouds length:', len(clouds)) \n",
    "print('radiation length:', len(radiation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aacc7e30-f280-4d94-9fa4-0282ad97ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clouds = clouds.reset_index().drop_duplicates()\n",
    "# clouds = clouds.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bcf73af-2a7d-4859-bea9-eb07bc172391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new length clouds: 341881\n"
     ]
    }
   ],
   "source": [
    "print('new length clouds:', len(clouds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27917263-f94f-4348-b7d7-39efc9a54e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- merge together:\n",
    "\n",
    "allstuff = temps.join([precip, radiation, clouds])\n",
    "\n",
    "# ----- convert units: \n",
    "\n",
    "#convert temperature K to C\n",
    "allstuff['t2m']=allstuff.t2m-273.15\n",
    "# precipotation m to mm \n",
    "allstuff['tp']=allstuff.tp * 1000\n",
    "# radiation j/m2 to w/m2\n",
    "# SSR [W/m2] = SSR [J/m^2] / (3600 seconds)\n",
    "allstuff['ssrd'] = allstuff.ssrd / 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be68629a-a8d8-4285-abc0-5698f22e1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allstuff.head()\n",
    "# allstuff[1000:1500].t2m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe3619ec-2ac4-455f-99c0-24df35ee7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns: \n",
    "\n",
    "allstuff = allstuff.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8ec997b-981e-4709-8565-cd2f14847bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "allstuff = allstuff.rename(columns={'time': 'D', 't2m':'Ta', 'tp':'Pr', 'tcc':'N', 'ssrd':'Rsw'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11303a78-6d6b-44e5-93fe-a9dda0f84439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>Ta</th>\n",
       "      <th>Pr</th>\n",
       "      <th>Rsw</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-01-01 00:00:00</td>\n",
       "      <td>-12.92365</td>\n",
       "      <td>0.031281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-01-01 01:00:00</td>\n",
       "      <td>-14.07618</td>\n",
       "      <td>0.022126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-01-01 02:00:00</td>\n",
       "      <td>-14.28500</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-01-01 03:00:00</td>\n",
       "      <td>-14.21647</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>30.036458</td>\n",
       "      <td>0.745823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-01-01 04:00:00</td>\n",
       "      <td>-13.98212</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>124.522639</td>\n",
       "      <td>0.621427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     D        Ta        Pr         Rsw         N\n",
       "0  1980-01-01 00:00:00 -12.92365  0.031281    0.000000  0.842461\n",
       "1  1980-01-01 01:00:00 -14.07618  0.022126    0.000000  0.782384\n",
       "2  1980-01-01 02:00:00 -14.28500  0.015259    0.000000  0.743381\n",
       "3  1980-01-01 03:00:00 -14.21647  0.012208   30.036458  0.745823\n",
       "4  1980-01-01 04:00:00 -13.98212  0.009155  124.522639  0.621427"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allstuff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa4cd7a7-78a6-471a-8456-0d56f370f6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>Pr</th>\n",
       "      <th>Ta</th>\n",
       "      <th>Rsw</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-10-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-10-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-10-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-10-01 03:00:00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-10-01 04:00:00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     D   Pr   Ta  Rsw   N\n",
       "0  1999-10-01 00:00:00  0.0  5.6  1.0 NaN\n",
       "1  1999-10-01 01:00:00  0.0  5.5  2.0 NaN\n",
       "2  1999-10-01 02:00:00  0.0  5.7  1.0 NaN\n",
       "3  1999-10-01 03:00:00  0.4  5.7  0.0 NaN\n",
       "4  1999-10-01 04:00:00  0.1  5.7  0.0 NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleclimate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab185f5-e260-43ad-8b4e-5761485ff860",
   "metadata": {},
   "source": [
    "### save output! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44d7f569-2c3e-4895-8e23-347775ad83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allstuff.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/19_Eastern_Tien_Shan/climate.met', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c378818-15fb-4a77-99b4-9f7b828fb374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project1] *",
   "language": "python",
   "name": "conda-env-project1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
