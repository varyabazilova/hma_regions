{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61688b3c-e40c-4200-9cb7-19da7868736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook varya is trying to export the median point (from the precipitation) from HMA - regions \n",
    "# to a dataframe and then later csv file to use as a forsing for testrun of sedcas model \n",
    "\n",
    "# take the locations determined in a different notebook (also stored in some csv file already)\n",
    "# loop through every file that is in the folder on the server:\n",
    "#    find location. select data \n",
    "#    \n",
    "\n",
    "# find that location in the HMA climate from the server \n",
    "# convert to csv \n",
    "# save \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e012c353-5572-49b5-8259-f5f4fd3c7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a090d2-dfe0-4526-b919-633c1c7d1c6a",
   "metadata": {},
   "source": [
    "## workapathround (make netcdf conversion to df faster): \n",
    "\n",
    "### (this is an important part) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b2b2d-ebad-4ce4-a590-04e2a45bb8ad",
   "metadata": {},
   "source": [
    "### locations from csv and .shp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df80741f-9a03-4f1d-8aea-95527e5d147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read locations file - for annual sum precipitations: \n",
    "locations_pr = pd.read_table('out/median_annual_precip_regions.csv', sep = ',',index_col =0)\n",
    "\n",
    "# read locations file - for annial median temp:\n",
    "locations_t2m = pd.read_table('out/annual_median_t2m_regions.csv', sep = ',', index_col = 0)\n",
    "\n",
    "# read hma polygons file to a df:\n",
    "hma = gpd.read_file('HMA_regions/HMA_regions.shp')\n",
    "hma_df = pd.DataFrame(hma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6817ec-e659-44d0-99e1-b0cea8fde8e3",
   "metadata": {},
   "source": [
    "### path to folders and output to where save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16e95e5-7cee-4d32-8635-c9771d8c544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_temp = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/2m-temperature/'\n",
    "\n",
    "# precipitations\n",
    "path_precip = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/'\n",
    "\n",
    "# total cloud cover \n",
    "path_clouds = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-cloud-cover/'\n",
    "\n",
    "# incoming radiation \n",
    "path_radiation = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/surface-solar-radiation-downwards/'\n",
    "\n",
    "\n",
    "# path for output timeseries\n",
    "path_output = \"/Users/varyabazilova/Desktop/hma_regions/out/timeseries\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad97a3-ac02-49e2-83f8-d18e22d9ef9c",
   "metadata": {},
   "source": [
    "### what mountain range are we looking at? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6220775-9960-44c8-b213-54b285507b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of mnt range: Western Himalaya\n"
     ]
    }
   ],
   "source": [
    "# take the n index of the \"locations precipitation\" table \n",
    "\n",
    "n = 1\n",
    "print('name of mnt range:', hma_df.Name[n])\n",
    "\n",
    "point_lat = locations_pr.latitude[n]\n",
    "point_lon = locations_pr.longitude[n]\n",
    "\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3682942-ccc0-4978-aa9d-22f0d2f59dcc",
   "metadata": {},
   "source": [
    "### loop: get the csv for selected point (one mnt range) for each parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8df821d1-1e60-40ea-966a-7f119fc4522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# temperatures\\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_temp)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_temp_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# temperatures\n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_temp)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_temp_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3493647-2c82-44f6-b13c-bf8cc8c07357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to turn into df at: 2022-06-20 11:19:44.589861\n",
      "CPU times: user 10.5 ms, sys: 12.4 ms, total: 22.9 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "# test cell to see what am i doing wrong :/ \n",
    "'''\n",
    "%%time \n",
    "preciptest = xr.open_dataset('/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/era5_total-precipitation_hourly_1979.nc')\n",
    "# preciptest2 = xr.open_dataset('/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/era5_total-precipitation_hourly_1980.nc')\n",
    "\n",
    "\n",
    "# ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "ds = preciptest.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2de1515c-7d7b-4659-b4c1-ef250c33e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = ds_df.reset_index()#.time.values\n",
    "\n",
    "# for x in preciptest.time.values:\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ee1d3c0-59b4-4124-97f1-b98b2bf39570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_df = ds_df.sort_values('time') \n",
    "# for x in ds_df.time.values:\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733ea5d-f495-46ef-a5e6-b31e8a746e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f132422-fcff-447b-b082-d6d529fe6fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# precipitation \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_precip)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\'])#.squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_precip_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# precipitation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_precip)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude'])#.squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_precip_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    " '''   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf3ffdde-f91e-4aae-a34c-c1fcf93dad2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time\\n\\n# clouds \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_clouds)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_cloud_cover_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# clouds \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_clouds)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_cloud_cover_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d80e5d3-3802-4f0f-83a4-09445b503d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# radiation \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_radiation)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_SWradiation_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n\\n '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# radiation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_radiation)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna()#.drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_SWradiation_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44af4f5-a22e-4a8e-9a31-f304055b9566",
   "metadata": {
    "tags": []
   },
   "source": [
    "### read created .csv-s and attach them together (separately for each variable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57cbf68b-9bb1-4f77-a8a2-8646d700e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 temperatures \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/temp/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "temps_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# temps_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/temps_1979_2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32ef846e-fc95-410f-a440-a68b84d5be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 precipitation \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/precip/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "precip_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# precip_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/precip_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b86c605-2c56-482b-8293-4b75ea4b0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 clouds \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/clouds/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "clouds_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# clouds_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/clouds_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bc6271c-ec5f-4f57-946e-eb28b9ee0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 radiation \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/radiation/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "swradiation_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# swradiation_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/SWradiation_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389646e-1430-4f29-bdee-14a8acc664ff",
   "metadata": {},
   "source": [
    "### merge all variables together using 'time' as key \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16e307b8-4307-4884-9b0d-62cbca9e305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample climate data - how it all is supposed to look like:\n",
    "\n",
    "sampleclimate = pd.read_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/sample_climate.met', sep = '\\t')\n",
    "# sampleclimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c50d26d-7a4a-43cc-8470-5f8f42e5a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all \n",
    "path = '/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya'\n",
    "\n",
    "temps = pd.read_csv(path + '/temps_1979_2020.csv', index_col = 0)\n",
    "precip = pd.read_csv(path + '/precip_1979_2020.csv', index_col = 0)\n",
    "clouds = pd.read_csv(path + '/clouds_1979_2020.csv', index_col = 0)\n",
    "radiation = pd.read_csv(path + '/SWradiation_1979_2020.csv', index_col = 0)\n",
    "\n",
    "# slice data so it starts and ends at the same time\n",
    "start = '1990-01-01 00:00:00' \n",
    "end = '2010-01-01 00:00:00'\n",
    "\n",
    "temps = temps.loc[start:end,:]\n",
    "precip = precip.loc[start:end,:]\n",
    "clouds = clouds.loc[start:end,:]\n",
    "radiation = radiation.loc[start:end,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bc5c536-b5d0-4f84-8084-fc2d19a0e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps length: 175321\n",
      "precip length: 175321\n",
      "clouds length: 350642\n",
      "radiation length: 175321\n"
     ]
    }
   ],
   "source": [
    "print('temps length:', len(temps)) \n",
    "print('precip length:', len(precip)) \n",
    "print('clouds length:', len(clouds)) \n",
    "print('radiation length:', len(radiation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b87047ea-1550-4305-89e5-d4ddaf7ae201",
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = clouds.reset_index().drop_duplicates()\n",
    "clouds = clouds.set_index('time').drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409d756-8457-409d-bc33-932f80dc98e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1bcf73af-2a7d-4859-bea9-eb07bc172391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new length clouds: 175321\n"
     ]
    }
   ],
   "source": [
    "print('new length clouds:', len(clouds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27917263-f94f-4348-b7d7-39efc9a54e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- merge together:\n",
    "\n",
    "allstuff = temps.join([precip, radiation, clouds])\n",
    "\n",
    "# ----- convert units: \n",
    "\n",
    "#convert temperature K to C\n",
    "allstuff['t2m']=allstuff.t2m-273.15\n",
    "# precipotation m to mm \n",
    "allstuff['tp']=allstuff.tp * 1000\n",
    "# radiation j/m2 to w/m2\n",
    "# SSR [W/m2] = SSR [J/m^2] / (3600 seconds)\n",
    "allstuff['ssrd'] = allstuff.ssrd / 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "be68629a-a8d8-4285-abc0-5698f22e1613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t2m</th>\n",
       "      <th>tp</th>\n",
       "      <th>ssrd</th>\n",
       "      <th>tcc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-01-01 00:00:00</th>\n",
       "      <td>3.80975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.891352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 01:00:00</th>\n",
       "      <td>3.91230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 02:00:00</th>\n",
       "      <td>4.10635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.905696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 03:00:00</th>\n",
       "      <td>4.53863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.186806</td>\n",
       "      <td>0.952360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 04:00:00</th>\n",
       "      <td>4.38085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.928958</td>\n",
       "      <td>0.863122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t2m   tp        ssrd       tcc\n",
       "time                                                   \n",
       "1990-01-01 00:00:00  3.80975  0.0   -0.000347  0.891352\n",
       "1990-01-01 01:00:00  3.91230  0.0   -0.000347  0.910000\n",
       "1990-01-01 02:00:00  4.10635  0.0   -0.000347  0.905696\n",
       "1990-01-01 03:00:00  4.53863  0.0   18.186806  0.952360\n",
       "1990-01-01 04:00:00  4.38085  0.0  101.928958  0.863122"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allstuff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe3619ec-2ac4-455f-99c0-24df35ee7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns: \n",
    "\n",
    "allstuff = allstuff.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a8ec997b-981e-4709-8565-cd2f14847bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "allstuff = allstuff.rename(columns={'time': 'D', 't2m':'Ta', 'tp':'Pr', 'tcc':'N', 'ssrd':'Rsw'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11303a78-6d6b-44e5-93fe-a9dda0f84439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>Ta</th>\n",
       "      <th>Pr</th>\n",
       "      <th>Rsw</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-01 00:00:00</td>\n",
       "      <td>3.80975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.891352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-01-01 01:00:00</td>\n",
       "      <td>3.91230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-01-01 02:00:00</td>\n",
       "      <td>4.10635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.905696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-01-01 03:00:00</td>\n",
       "      <td>4.53863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.186806</td>\n",
       "      <td>0.952360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-01-01 04:00:00</td>\n",
       "      <td>4.38085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.928958</td>\n",
       "      <td>0.863122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     D       Ta   Pr         Rsw         N\n",
       "0  1990-01-01 00:00:00  3.80975  0.0   -0.000347  0.891352\n",
       "1  1990-01-01 01:00:00  3.91230  0.0   -0.000347  0.910000\n",
       "2  1990-01-01 02:00:00  4.10635  0.0   -0.000347  0.905696\n",
       "3  1990-01-01 03:00:00  4.53863  0.0   18.186806  0.952360\n",
       "4  1990-01-01 04:00:00  4.38085  0.0  101.928958  0.863122"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allstuff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aa4cd7a7-78a6-471a-8456-0d56f370f6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>Pr</th>\n",
       "      <th>Ta</th>\n",
       "      <th>Rsw</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-10-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-10-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-10-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-10-01 03:00:00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-10-01 04:00:00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     D   Pr   Ta  Rsw   N\n",
       "0  1999-10-01 00:00:00  0.0  5.6  1.0 NaN\n",
       "1  1999-10-01 01:00:00  0.0  5.5  2.0 NaN\n",
       "2  1999-10-01 02:00:00  0.0  5.7  1.0 NaN\n",
       "3  1999-10-01 03:00:00  0.4  5.7  0.0 NaN\n",
       "4  1999-10-01 04:00:00  0.1  5.7  0.0 NaN"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleclimate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab185f5-e260-43ad-8b4e-5761485ff860",
   "metadata": {},
   "source": [
    "### save output! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44d7f569-2c3e-4895-8e23-347775ad83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allstuff.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western_Himalaya/climate.met', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c378818-15fb-4a77-99b4-9f7b828fb374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
