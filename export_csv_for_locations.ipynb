{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61688b3c-e40c-4200-9cb7-19da7868736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook varya is trying to export the median point (from the precipitation) from HMA - regions \n",
    "# to a dataframe and then later csv file to use as a forsing for testrun of sedcas model \n",
    "\n",
    "# take the locations determined in a different notebook (also stored in some csv file already)\n",
    "# loop through every file that is in the folder on the server:\n",
    "#    find location. select data \n",
    "#    \n",
    "\n",
    "# find that location in the HMA climate from the server \n",
    "# convert to csv \n",
    "# save \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e012c353-5572-49b5-8259-f5f4fd3c7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a090d2-dfe0-4526-b919-633c1c7d1c6a",
   "metadata": {},
   "source": [
    "## workapathround (make netcdf conversion to df faster): \n",
    "\n",
    "### (this is an important part) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b2b2d-ebad-4ce4-a590-04e2a45bb8ad",
   "metadata": {},
   "source": [
    "### locations from csv and .shp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df80741f-9a03-4f1d-8aea-95527e5d147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read locations file - for annual sum precipitations: \n",
    "locations_pr = pd.read_table('out/median_annual_precip_regions.csv', sep = ',',index_col =0)\n",
    "\n",
    "# read locations file - for annial median temp:\n",
    "locations_t2m = pd.read_table('out/annual_median_t2m_regions.csv', sep = ',', index_col = 0)\n",
    "\n",
    "# read hma polygons file to a df:\n",
    "hma = gpd.read_file('HMA_regions/HMA_regions.shp')\n",
    "hma_df = pd.DataFrame(hma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6817ec-e659-44d0-99e1-b0cea8fde8e3",
   "metadata": {},
   "source": [
    "### path to folders and output to where save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16e95e5-7cee-4d32-8635-c9771d8c544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_temp = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/2m-temperature/'\n",
    "\n",
    "# precipitations\n",
    "path_precip = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/'\n",
    "\n",
    "# total cloud cover \n",
    "path_clouds = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-cloud-cover/'\n",
    "\n",
    "# incoming radiation \n",
    "path_radiation = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/surface-solar-radiation-downwards/'\n",
    "\n",
    "\n",
    "# path for output timeseries\n",
    "path_output = \"/Users/varyabazilova/Desktop/hma_regions/out/timeseries\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad97a3-ac02-49e2-83f8-d18e22d9ef9c",
   "metadata": {},
   "source": [
    "### what mountain range are we looking at? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6220775-9960-44c8-b213-54b285507b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of mnt range: Tibetan Interior Mountains\n"
     ]
    }
   ],
   "source": [
    "# take the n index of the \"locations precipitation\" table \n",
    "\n",
    "n = 13\n",
    "print('name of mnt range:', hma_df.Name[n])\n",
    "\n",
    "point_lat = locations_pr.latitude[n]\n",
    "point_lon = locations_pr.longitude[n]\n",
    "\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3682942-ccc0-4978-aa9d-22f0d2f59dcc",
   "metadata": {},
   "source": [
    "### loop: get the csv for selected point (one mnt range) for each parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df821d1-1e60-40ea-966a-7f119fc4522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# temperatures\\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_temp)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output_temp + \\'_temp_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# temperatures\n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_temp)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_temp_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    " '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f132422-fcff-447b-b082-d6d529fe6fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# precipitation \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_precip)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_precip_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# precipitation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_precip)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_precip_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    "''' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3ffdde-f91e-4aae-a34c-c1fcf93dad2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# clouds \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_clouds)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_cloud_cover_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# clouds \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_clouds)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_cloud_cover_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    " '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d80e5d3-3802-4f0f-83a4-09445b503d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# radiation \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_radiation)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_SWradiation_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# radiation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_radiation)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_SWradiation_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    '''\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44af4f5-a22e-4a8e-9a31-f304055b9566",
   "metadata": {
    "tags": []
   },
   "source": [
    "### read created .csv-s and attach them together (separately for each variable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "57cbf68b-9bb1-4f77-a8a2-8646d700e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 temperatures \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/temp/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "temps_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# temps_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/temps_1979_2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "91c69801-2cd5-4fcd-a091-359bb02db38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temps_region1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32ef846e-fc95-410f-a440-a68b84d5be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 precipitation \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/precip/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "precip_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# precip_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/precip_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7b86c605-2c56-482b-8293-4b75ea4b0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 clouds \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/clouds/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "clouds_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# clouds_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/clouds_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97c055-c617-4cbe-97d0-85a0f4e82b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2bc6271c-ec5f-4f57-946e-eb28b9ee0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region #1 radiation \n",
    "\n",
    "files = glob.glob('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/radiation/*.csv')\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_all.append(df)\n",
    "\n",
    "swradiation_region1 = pd.concat(df_all).sort_values('time').drop('expver', axis=1).set_index('time')\n",
    "# swradiation_region1.to_csv('/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya/SWradiation_1979_2020.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389646e-1430-4f29-bdee-14a8acc664ff",
   "metadata": {},
   "source": [
    "### merge all variables together using 'time' as key \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2c50d26d-7a4a-43cc-8470-5f8f42e5a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all \n",
    "path = '/Users/varyabazilova/Desktop/hma_regions/out/timeseries/1_Western Himalaya'\n",
    "\n",
    "temps = pd.read_csv(path + '/temps_1979_2020.csv')\n",
    "precip = pd.read_csv(path + '/precip_1979_2020.csv')\n",
    "clouds = pd.read_csv(path + '/clouds_1979_2020.csv')\n",
    "radiation = pd.read_csv(path + '/SWradiation_1979_2020.csv')\n",
    "\n",
    "# merge all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27917263-f94f-4348-b7d7-39efc9a54e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cd7a7-78a6-471a-8456-0d56f370f6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
