{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61688b3c-e40c-4200-9cb7-19da7868736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook varya is trying to export the median point (from the precipitation) from HMA - regions \n",
    "# to a dataframe and then later csv file to use as a forsing for testrun of sedcas model \n",
    "\n",
    "# take the locations determined in a different notebook (also stored in some csv file already\n",
    "# find that location in the HMA climate from the server \n",
    "# convert to csv \n",
    "# save \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012c353-5572-49b5-8259-f5f4fd3c7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf49e7b-a43a-4972-a9b5-efc2b18e840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read locations file - for annual sum precipitations: \n",
    "locations_pr = pd.read_table('out/median_annual_precip_regions.csv', sep = ',',index_col =0)\n",
    "\n",
    "# read locations file - for annial median temp:\n",
    "locations_t2m = pd.read_table('out/annual_median_t2m_regions.csv', sep = ',', index_col = 0)\n",
    "\n",
    "# read hma polygons file to a df:\n",
    "hma = gpd.read_file('HMA_regions/HMA_regions.shp')\n",
    "hma_df = pd.DataFrame(hma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da593243-30e8-4967-a7e7-c3657dc30f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations_pr.head()\n",
    "# locations_t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e39d5-cec0-4bb5-9a74-f1ef8348b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ----------- read climate data: ----------- \n",
    "\n",
    "# temperatures\n",
    "temps = xr.open_mfdataset(\"/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/2m-temperature/*.nc\")\n",
    "\n",
    "# precipitations\n",
    "precip = xr.open_mfdataset(\"/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/*.nc\")\n",
    "\n",
    "# total cloud cover \n",
    "clouds = xr.open_mfdataset(\"/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-cloud-cover/*.nc\")\n",
    "\n",
    "# incoming radiation \n",
    "radiation = xr.open_mfdataset(\"/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/surface-solar-radiation-downwards/*.nc\")\n",
    "\n",
    "\n",
    "# path for output timeseries\n",
    "path_output = \"/Users/varyabazilova/Desktop/hma_regions/out/timeseries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a7f21-a613-44ec-b2b4-f410f6e980bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(temps.t2m.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f3caa-af76-4373-9ea8-f0ab9dd696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- convert units: ----------- \n",
    "\n",
    "# climate = xr.open_mfdataset('*.nc', decode_coords=\"all\")\n",
    "# cnange units (and overwrite the metadata with the units after)\n",
    "\n",
    "# ------ UNITS ------\n",
    "\n",
    "#convert temperature K to C\n",
    "temps['t2m']=temps.t2m-273.15\n",
    "# precipotation m to mm \n",
    "precip['tp']=precip.tp * 1000\n",
    "# radiation j/m2 to w/m2\n",
    "# SSR [W/m2] = SSR [J/m^2] / (3600 seconds)\n",
    "radiation['ssrd'] = radiation.ssrd / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f00a4-de66-45ad-87f4-ca2a5d1f7ac8",
   "metadata": {},
   "source": [
    "### (1) select coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6e424-52d1-476c-b331-33146bfe2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the n index of the \"locations precipitation\" table \n",
    "\n",
    "n = 13\n",
    "print('name of mnt range:', hma_df.Name[n])\n",
    "\n",
    "point_lat = locations_pr.latitude[n]\n",
    "point_lon = locations_pr.longitude[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea286c8-221b-46b3-9c0f-a9cce3a06cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# kick out unnecessary dimention:\n",
    "temps = temps.sel(expver= slice(0, 1))\n",
    "precip = precip.sel(expver = slice(0,1))\n",
    "clouds = clouds.sel(expver = slice(0,1))\n",
    "radiation = radiation.sel(expver = slice(0,1))\n",
    "\n",
    "# select the point in the each dataset \n",
    "\n",
    "temps_point = temps.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "precip_point = precip.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "clouds_point = clouds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "radiation_point = radiation.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "\n",
    "# make a time slice so that it dpesnt take THAT much long: \n",
    "temps_point = temps_point.sel(time=slice(\"1979-01-01\", \"1980-01-01\")).drop(labels = ['longitude', 'latitude']).squeeze()#.drop(label = 'expver')\n",
    "\n",
    "# drop coordinates dimention: \n",
    "# temps_point = temps_point.drop(labels = ['longitude', 'latitude', 'expver'])\n",
    "\n",
    "# temps_point1 = temps_point.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc034ac1-b424-4e9b-af50-94a7c1dd0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temps_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67160d2e-876f-4e88-aae4-b81ee3780b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = temps_point.to_dataframe()#.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4aeb4-5b43-4372-8c4c-f75f1d768aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3af46-9ce4-4be5-be13-a60b6c1feb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# df = temps_point1.to_dataframe().dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa22c6-30ec-4583-ba01-00ed1229e08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518a3f4-7972-40f8-997c-d2ce327ebfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "temps_point1_df = temps_point1.to_dataframe().dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff3177-84be-4f3f-8475-9fb7923b8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ----------- convert to dataframe and cleanup: ----------- \n",
    "\n",
    "# temps_point_df = temps_point.to_dataframe()\n",
    "\n",
    "# temps_point1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39085b48-c06e-4f20-b3f2-b0138242e41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303fdfc-7eff-49f2-b4e7-c4f22744407b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442676d0-eba5-4df1-a0b1-419c84daa393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8decb92-6614-422c-9932-1450adefdfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f8e34-4dd6-4a46-a8e8-6707189794b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481e9db-424c-48c3-9780-f853ab8afdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temps_point\n",
    "len(temps.time.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a090d2-dfe0-4526-b919-633c1c7d1c6a",
   "metadata": {},
   "source": [
    "## workapathround (make netcdf conversion to df faster): \n",
    "\n",
    "### (this is an important part) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7782a8fd-f0f9-4962-af90-122bcc910692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df80741f-9a03-4f1d-8aea-95527e5d147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read locations file - for annual sum precipitations: \n",
    "locations_pr = pd.read_table('out/median_annual_precip_regions.csv', sep = ',',index_col =0)\n",
    "\n",
    "# read locations file - for annial median temp:\n",
    "locations_t2m = pd.read_table('out/annual_median_t2m_regions.csv', sep = ',', index_col = 0)\n",
    "\n",
    "# read hma polygons file to a df:\n",
    "hma = gpd.read_file('HMA_regions/HMA_regions.shp')\n",
    "hma_df = pd.DataFrame(hma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbec9a-38a9-4405-a5af-2ad124e45bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fb10b-61f6-482f-9f4f-3a3345d15e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16e95e5-7cee-4d32-8635-c9771d8c544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_temp = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/2m-temperature/'\n",
    "\n",
    "# precipitations\n",
    "path_precip = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-precipitation/'\n",
    "\n",
    "# total cloud cover \n",
    "path_clouds = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/total-cloud-cover/'\n",
    "\n",
    "# incoming radiation \n",
    "path_radiation = '/Volumes/Data/Repository/external_data/ERA5/HMA/netcdf/hourly/surface-solar-radiation-downwards/'\n",
    "\n",
    "\n",
    "# path for output timeseries\n",
    "path_output_temp = \"/Users/varyabazilova/Desktop/hma_regions/out/timeseries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6220775-9960-44c8-b213-54b285507b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of mnt range: Tibetan Interior Mountains\n"
     ]
    }
   ],
   "source": [
    "# take the n index of the \"locations precipitation\" table \n",
    "\n",
    "n = 13\n",
    "print('name of mnt range:', hma_df.Name[n])\n",
    "\n",
    "point_lat = locations_pr.latitude[n]\n",
    "point_lon = locations_pr.longitude[n]\n",
    "\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df821d1-1e60-40ea-966a-7f119fc4522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# temperatures\\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_temp)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output_temp + \\'_temp_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# temperatures\n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_temp)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_temp, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output_temp + '_temp_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    " '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d6661-fbc8-4644-b741-b3b79dcb1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read all the csv files created \n",
    "\n",
    "# append from the bottom \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24861a49-9073-43ba-8fe9-a03ca479a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for output timeseries\n",
    "path_output = \"/Users/varyabazilova/Desktop/hma_regions/out/timeseries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f132422-fcff-447b-b082-d6d529fe6fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# precipitation \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_precip)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_precip_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# precipitation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_precip)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_precip, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_precip_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    "''' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9c3e9-49d5-4db0-b2f5-67622b6e77f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3ffdde-f91e-4aae-a34c-c1fcf93dad2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# clouds \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_clouds)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_cloud_cover_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    \\n '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# clouds \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_clouds)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_clouds, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_cloud_cover_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    \n",
    " '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d80e5d3-3802-4f0f-83a4-09445b503d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\n\\n# radiation \\n\\nprint(\\'started at:\\', datetime.now())\\n\\nfor n, f in enumerate(os.listdir(path_radiation)):\\n    # read every file\\n    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\\n    # select coordinates: \\n    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = \\'nearest\\')\\n    # kick out the expver dimention\\n    # ds = ds.sel(expver= slice(0, 1))\\n    # ds = ds.squeeze()\\n    ds = ds.drop(labels = [\\'longitude\\', \\'latitude\\']).squeeze()\\n    \\n    # check:\\n    print(\\'ready to turn into df at:\\', datetime.now())\\n    \\n    # convert to dataframe\\n    ds_df = ds.to_dataframe().dropna().drop_duplicates()\\n    \\n    # export:\\n    # 1. create a year \\n    year = n + 1979\\n    # 2. save to csv\\n    ds_df.to_csv(path_output + \\'_SWradiation_{y}.csv\\'.format(y=year))\\n    \\n    print(\\'year - done:\\', year)\\n    print(\\'done at:\\', datetime.now())\\n    '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# radiation \n",
    "\n",
    "print('started at:', datetime.now())\n",
    "\n",
    "for n, f in enumerate(os.listdir(path_radiation)):\n",
    "    # read every file\n",
    "    ds = xr.open_dataset(os.path.join(path_radiation, f), decode_coords=\"all\")\n",
    "    # select coordinates: \n",
    "    ds = ds.sel(latitude = point_lat, longitude = point_lon, method = 'nearest')\n",
    "    # kick out the expver dimention\n",
    "    # ds = ds.sel(expver= slice(0, 1))\n",
    "    # ds = ds.squeeze()\n",
    "    ds = ds.drop(labels = ['longitude', 'latitude']).squeeze()\n",
    "    \n",
    "    # check:\n",
    "    print('ready to turn into df at:', datetime.now())\n",
    "    \n",
    "    # convert to dataframe\n",
    "    ds_df = ds.to_dataframe().dropna().drop_duplicates()\n",
    "    \n",
    "    # export:\n",
    "    # 1. create a year \n",
    "    year = n + 1979\n",
    "    # 2. save to csv\n",
    "    ds_df.to_csv(path_output + '_SWradiation_{y}.csv'.format(y=year))\n",
    "    \n",
    "    print('year - done:', year)\n",
    "    print('done at:', datetime.now())\n",
    "    '''\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d316219-8bd0-4fe6-a879-e7e21397a80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f2579-ae8a-4433-a591-486aa91e332a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db74dae-48b5-44c3-a9b9-7fd3c5b65f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
